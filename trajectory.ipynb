{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Welcome to this Jupyter Notebook, where we will be conducting an analysis of GPS data provided by Perigon AI, covering Yerevan for the <br>\n",
    "years 2019-2022. The dataset includes user_id, latitude, longitude, and timestamp information. Our primary objective is to analyze movement <br>\n",
    "patterns and parking behaviors in Yerevan using advanced data processing and segmentation techniques. We will utilize several Python <br>\n",
    "libraries such as `NumPy`, `Pandas`, `GeoPandas`, `Shapely`, `PyProj`, `OSMnx`, `NetworkX`, and Matplotlib for data manipulation and <br>\n",
    "analysis, along with Trackintel and custom libraries for processing, segmentation and more tasks.\n",
    "\n",
    "In the following sections, we will import the necessary libraries and start by cleaning and preparing the dataset for analysis. The <br>\n",
    "analysis will focus on deriving staypoints from positionfixes, performing trajectory segmentation, and visualizing the results to <br>\n",
    "gain insights into mobility patterns within the city. The use of `tqdm` will help track the progress of our computations, and <br>\n",
    "warnings will be suppressed for a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import trackintel as ti\n",
    "\n",
    "import lib.process as prcs\n",
    "import lib.segmentation as seg\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GPS Data Pre-Processing\n",
    "\n",
    "In the context of our study on parking pattern analysis in Yerevan, the GPS data preprocessing stage is fundamental <br>\n",
    "to refining the raw location data sourced from smartphones. This crucial step involves meticulous cleaning and validation <br>\n",
    "procedures to enhance the accuracy of the dataset. Tasks such as handling missing or inaccurate data points, filtering out <br>\n",
    "anomalies, and standardizing data formats are integral to ensuring the reliability of the information. The effectiveness of <br>\n",
    "the parking pattern analysis heavily relies on a well-executed preprocessing stage, setting the groundwork for meaningful <br>\n",
    "insigts into the dynamics of parking behavior within the city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Loading and Combining Data\n",
    "In this section, we will load the position fixes from multiple CSV files and combine them into a single DataFrame. <br> \n",
    "The data files are stored in the `./data/sample/` directory and are named sequentially from `sample_0.csv` to `sample_18.csv`. <br>\n",
    "We will use a loop to read each file, append it to a list, and then concatenate all the DataFrames into one. This will create <br>\n",
    "a comprehensive dataset containing position fixes for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = []\n",
    "\n",
    "for i in range(19):\n",
    "    pos = prcs.read_positionfixes(f'./data/sample/sample_{i}.csv')\n",
    "    pfs.append(pos)\n",
    "\n",
    "pfs = pd.concat(pfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Cleaning and Filtering Data\n",
    "After loading and combining the position fixes, we will perform data cleaning to remove any duplicates and filter <br>\n",
    "the points to retain only those within Yerevan's boundaries. This ensures the accuracy and relevance of our dataset <br>\n",
    "for further analysis. The cleaned data will be stored in a CSV file under the name `./data./positionfixes.csv` and read <br>\n",
    "into a trackintel positionfixes structure for standardized processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates if any\n",
    "pfs.drop_duplicates()\n",
    "\n",
    "# Extracting points that are in Yerevan's polygon\n",
    "pfs = prcs.filter_yerevan_data(pfs)\n",
    "\n",
    "# Store the positionfixes in a csv\n",
    "pfs.to_csv('./data/positionfixes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the positionfixes extracted from the sample data\n",
    "pfs = []\n",
    "\n",
    "for i in range(7):\n",
    "    pfs.append(ti.io.read_positionfixes_csv(f'./data/positionfixes/positionfixes_{i}.csv', sep=\",\", tz='UTC', index_col=0, crs=prcs.CRS.from_epsg(4326)))\n",
    "\n",
    "pfs = pd.concat(pfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting People\n",
    "After filtering and cleaning the data, the next step involves grouping the data according to each device. This allows <br>\n",
    "us to analyze the movement patterns of individual users and extract valuable information. By tracking each device individually, <br>\n",
    "we can calculate movement modes and construct trips. Additionally, it helps in understanding trends and preferences that may <br>\n",
    "inform urban planning and transportation policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating by each device id\n",
    "people = prcs.extract_people(pfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Generating Staypoints\n",
    "After extraction we still need to do some cleaning in order to get rid of noisy and uninformative data.We utilize the <br>\n",
    "`generate_staypoints()` method of trackintel library to extract staypoints from the GPS data for each individual represented <br>\n",
    "by the people list. By iterating over each person and calling this method, we identify locations where the device stayed stationary <br>\n",
    "for a significant duration, indicative of potential destinations or points of interest.This process will clean all the days that are <br> \n",
    "unable to generating staypoints for people. By so we significantly decreased the points but we increased the accuracy by working with <br>\n",
    "a representative data.These staypoints are then stored for future analysis and reuse, contributing to a deeper understanding of individual <br>\n",
    "mobility patterns and urban behavior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SP Generated: 100%|\u001b[32m██████████\u001b[0m| 508478/508478 [1:17:39<00:00, 109.12it/s] \n"
     ]
    }
   ],
   "source": [
    "# Generate staypoints and store them in a file to reuse\n",
    "for person in tqdm(people, colour='GREEN', desc='SP Generated: '):\n",
    "    person.generate_staypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pfs and sp after generation of staypoints\n",
    "all_pfs = []\n",
    "all_sp = []\n",
    "\n",
    "for person in tqdm(people, colour='GREEN', desc='SP Generated: '):\n",
    "    all_pfs.append(person.pfs)\n",
    "    all_sp.append(person.sp)\n",
    "\n",
    "all_pfs = pd.concat(all_pfs, ignore_index=True)\n",
    "all_sp = pd.concat(all_sp, ignore_index=True)\n",
    "\n",
    "all_pfs.to_csv('./data/inf_positionfixes.csv')\n",
    "all_sp.to_csv('./data/inf_staypoints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Reading clean positionfixes and staypoins and updating people\n",
    "After our data cleaning we can precceed the work with already clean data. Let us get the positionfixes and staypoints <br>\n",
    "for further processing and trajectory formation. Thenwe can extract people and generate staypoints for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "People Extracted: 100%|\u001b[32m██████████\u001b[0m| 10021/10021 [00:12<00:00, 826.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reading positionfixes that have staypoints\n",
    "pfs_clean = [] \n",
    "\n",
    "for i in range(2):\n",
    "    pfs_clean.append(ti.io.read_positionfixes_csv(f'./data/positionfixes/inf_positionfixes_{i}.csv', sep=\",\", tz='UTC', index_col=0, crs=prcs.CRS.from_epsg(4326)))\n",
    "\n",
    "pfs_clean = pd.concat(pfs_clean, ignore_index=True)\n",
    "\n",
    "# Extracting people out of clean pfs\n",
    "people = prcs.extract_people(pfs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the staypoints that are already stored and putting them in peopel\n",
    "sp_clean = ti.io.read_staypoints_csv('./data/positionfixes/inf_staypoints.csv', sep=\",\", tz='UTC', index_col=0, crs=prcs.CRS.from_epsg(4326))\n",
    "prcs.update_staypoints(people, sp_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Micro-Segmentation\n",
    "Here we carry the process of segmenting GPS data into meaningful segments representing distinct movement patterns, <br>\n",
    "such as walking or driving. The `convert_to_segments` function takes a sequence of position fixes (pfs) and calculates <br>\n",
    "the distance, duration, and average speed between consecutive fixes. It also constructs a path through the street map <br>\n",
    "based on the fixes' geographical coordinates, providing insights into the route taken by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Storage of micro-segments\n",
    "As segment creation takes place for every consecutive points in daily basis the `save_segments` function is used <br>\n",
    "to save the segments generated from the GPS data for each user into CSV files, enabling storage and future analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segments(segments, index):\n",
    "    frames =  []\n",
    "\n",
    "    for item in segments:\n",
    "        for seg in item:\n",
    "            frame = pd.DataFrame(seg)\n",
    "        \n",
    "        frames.append(frame)\n",
    "    \n",
    "    s = pd.concat(frames, ignore_index=True)\n",
    "    s.to_csv(f'./data/segments/sample_{index}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Creation of segments\n",
    "\n",
    "Furthermore, the process of generating segments from the cleaned position fixes data for each user is demonstrated. <br>\n",
    "The combined segments from all users are read from the CSV files, concatenated into a single DataFrame, and written <br>\n",
    "to a new CSV file. This consolidated dataset provides a comprehensive overview of movement patterns across different <br>\n",
    "users, facilitating further analysis and insights into urban mobility behavior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10021/10021 [18:31:27<00:00,  6.65s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Trying to create the segments from the clean pfs data for each user\n",
    "segments = []\n",
    "\n",
    "for i in tqdm(range(len(people))):\n",
    "    pfs_days = people[i].group_pfs_by_date()\n",
    "    segments_day = []\n",
    "    \n",
    "    for day in pfs_days:\n",
    "        segment = seg.convert_to_segments(day)\n",
    "        segments_day.append(segment)\n",
    "    \n",
    "    segments.append(segments_day)\n",
    "\n",
    "    if (i % 25 == 0):\n",
    "        save_segments(segments, i / 25)\n",
    "        segments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiningthe output of segments\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file and append its DataFrame to the list\n",
    "for i in range(1, 401):\n",
    "    df = pd.read_csv(f'./data/segments/sample_{i}.0.csv')\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list along rows\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('./data/segments.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Retrieval of Segments\n",
    "The retrieval of the segments was a significant step, however we need to continue our processing. Let us retrieve <br>\n",
    "our segments for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the segment information\n",
    "from shapely.wkt import loads\n",
    "\n",
    "df = pd.read_csv('./data/segments.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "df['geom'] = df['geom'].apply(loads)\n",
    "segments = gpd.GeoDataFrame(df, geometry='geom', crs=prcs.CRS.from_epsg(4326))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9998/9998 [05:27<00:00, 30.55it/s] \n"
     ]
    }
   ],
   "source": [
    "# Merging the segments by first grouping segments by user and by date\n",
    "\n",
    "from tqdm import tqdm\n",
    "merged_segments = []\n",
    "adjusted_segments = []\n",
    "\n",
    "user_segments = seg.split_segments_by_user(segments)\n",
    "\n",
    "for user in tqdm(user_segments):\n",
    "    day_segments = seg.split_segments_by_date(user)\n",
    "    for segment in day_segments:\n",
    "        merged = seg.merge_segments(segment, v_thresh=0.5)\n",
    "        adjusted = seg.merge_segments(merged)\n",
    "        \n",
    "        merged_segments.append(merged)\n",
    "        adjusted_segments.append(adjusted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
